{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Wojcik\\Documents\\tensorflow\\pyspark-benchmark\\test.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Wojcik/Documents/tensorflow/pyspark-benchmark/test.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfindspark\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Wojcik/Documents/tensorflow/pyspark-benchmark/test.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m findspark\u001b[39m.\u001b[39minit()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Wojcik/Documents/tensorflow/pyspark-benchmark/test.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m Spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mmaster(\u001b[39m\"\u001b[39;49m\u001b[39mlocal[*]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mSparkByExamples.com\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n",
      "File \u001b[1;32mc:\\Users\\Wojcik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:272\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    270\u001b[0m     \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m--> 272\u001b[0m     session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_options)\n\u001b[0;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[39mgetattr\u001b[39m(\n\u001b[0;32m    275\u001b[0m         \u001b[39mgetattr\u001b[39m(session\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     )\u001b[39m.\u001b[39mapplyModifiableSettings(session\u001b[39m.\u001b[39m_jsparkSession, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\Wojcik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:307\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[1;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mapplyModifiableSettings(\n\u001b[0;32m    304\u001b[0m             jsparkSession, options\n\u001b[0;32m    305\u001b[0m         )\n\u001b[0;32m    306\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m         jsparkSession \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mSparkSession(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), options)\n\u001b[0;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mapplyModifiableSettings(\n\u001b[0;32m    310\u001b[0m         jsparkSession, options\n\u001b[0;32m    311\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Wojcik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1579\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1581\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1584\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1585\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1586\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[0;32m   1588\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1589\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\Wojcik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[0;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    335\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    336\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "Spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkByExamples.com\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"using\", device, \"device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ CPU SPEED ***************\n",
      "59.71805810928345\n",
      "verify device: cpu\n",
      "************ GPU SPEED ***************\n",
      "1.7862207889556885\n",
      "verify device: cuda:0\n",
      "************ GPU SPEED ***************\n",
      "1.81290864944458\n",
      "verify device: cuda:0\n",
      "************ GPU SPEED ***************\n",
      "1.8004319667816162\n",
      "verify device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "matrix_size = 32*600\n",
    "\n",
    "x = torch.randn(matrix_size, matrix_size)\n",
    "y = torch.randn(matrix_size, matrix_size)\n",
    "\n",
    "print(\"************ CPU SPEED ***************\")\n",
    "start = time.time()\n",
    "result = torch.matmul(x,y)\n",
    "print(time.time() - start)\n",
    "print(\"verify device:\", result.device)\n",
    "\n",
    "x_gpu = x.to(device)\n",
    "y_gpu = y.to(device)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"************ GPU SPEED ***************\")\n",
    "    start = time.time()\n",
    "    result_gpu = torch.matmul(x_gpu,y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    print(time.time() - start)\n",
    "    print(\"verify device:\", result_gpu.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-142.4877, -157.4479, -134.2422,  ...,   62.2809,    0.5038,\n",
       "            7.5595],\n",
       "        [ 219.8301,  136.9860,  201.6911,  ...,  -40.5193,  -12.2883,\n",
       "          152.4540],\n",
       "        [  50.6536, -106.3733, -110.4533,  ...,   23.4575,   70.9110,\n",
       "           31.7738],\n",
       "        ...,\n",
       "        [  -2.7893, -232.6013, -174.9835,  ...,   35.7527,  -63.3191,\n",
       "          -63.1375],\n",
       "        [-144.2040,   39.6278,  128.2334,  ...,  -52.6592, -101.1328,\n",
       "           -3.8151],\n",
       "        [-129.5593, -190.7631,  130.5468,  ..., -249.3604, -185.3102,\n",
       "          142.8233]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-142.4874, -157.4482, -134.2427,  ...,   62.2808,    0.5036,\n",
       "            7.5589],\n",
       "        [ 219.8308,  136.9859,  201.6908,  ...,  -40.5193,  -12.2881,\n",
       "          152.4539],\n",
       "        [  50.6536, -106.3734, -110.4541,  ...,   23.4573,   70.9108,\n",
       "           31.7742],\n",
       "        ...,\n",
       "        [  -2.7895, -232.6006, -174.9838,  ...,   35.7528,  -63.3191,\n",
       "          -63.1375],\n",
       "        [-144.2037,   39.6275,  128.2333,  ...,  -52.6593, -101.1328,\n",
       "           -3.8151],\n",
       "        [-129.5596, -190.7642,  130.5474,  ..., -249.3604, -185.3098,\n",
       "          142.8232]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFLow version: 2.10.1\n",
      "Num GPUs Available: 1\n",
      "Num CPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(\"TensorFLow version:\",tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available:\", len(tf.config.experimental.list_physical_devices('CPU')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n",
      "Num GPUs Available:  1\n",
      "WARNING:tensorflow:AutoGraph could not transform <function normalize_img at 0x0000026813CDF490> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x0000026813CDF490>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function normalize_img at 0x0000026813CDF490> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x0000026813CDF490>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function normalize_img at 0x0000026813CDF490> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x0000026813CDF490>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/12\n",
      "469/469 [==============================] - 4s 5ms/step - loss: 0.1490 - accuracy: 0.9546 - val_loss: 0.0662 - val_accuracy: 0.9774\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0427 - accuracy: 0.9872 - val_loss: 0.0379 - val_accuracy: 0.9875\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0264 - accuracy: 0.9918 - val_loss: 0.0402 - val_accuracy: 0.9860\n",
      "Epoch 4/12\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.0341 - val_accuracy: 0.9893\n",
      "Epoch 5/12\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0122 - accuracy: 0.9959 - val_loss: 0.0335 - val_accuracy: 0.9902\n",
      "Epoch 6/12\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.0337 - val_accuracy: 0.9893\n",
      "Epoch 7/12\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.0529 - val_accuracy: 0.9855\n",
      "Epoch 8/12\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0070 - accuracy: 0.9976 - val_loss: 0.0518 - val_accuracy: 0.9869\n",
      "Epoch 9/12\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.0440 - val_accuracy: 0.9894\n",
      "Epoch 10/12\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.0369 - val_accuracy: 0.9900\n",
      "Epoch 11/12\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0034 - accuracy: 0.9987 - val_loss: 0.0433 - val_accuracy: 0.9897\n",
      "Epoch 12/12\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0053 - accuracy: 0.9980 - val_loss: 0.0382 - val_accuracy: 0.9909\n",
      "CPU times: total: 1.83 s\n",
      "Wall time: 30.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x268127d8730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "batch_size = 128\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu'),\n",
    "  tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#   tf.keras.layers.Dropout(0.25),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "#   tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=12,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "722e6c385a996ac5857d6958fb44aed6d348e59b613f4820474c18e6098ecc84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
